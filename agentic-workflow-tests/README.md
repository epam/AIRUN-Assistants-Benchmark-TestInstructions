# Agentic Workflow Tests

> [!NOTE]
> Agentic test automation is under development. The testing procedure described below is subject to change.

## Setup

### Prerequisites

First, you need a unix-like terminal to run the tests. You can choose the one that suits you best. The paths below are shown as examples, adapt them to your needs.

### Automatic LLM evaluation framework

[The LLM evaluation framework](https://github.com/epam/AIRUN-Evaluation-Framework/tree/main) is used 
to evaluate solutions generated by coding agents.

The framework setup is described in the README in its [Run Environment Setup section](https://github.com/epam/AIRUN-Evaluation-Framework/tree/main#run-environment-setup). There are two options, the preffered way is *Windows/Unix+Python+venv*.


### Environment Setup


Set the framework environment:

```bash
# Two keys to access LLMs:

export AZURE_API_KEY=...

export OPENAI_API_KEY=...

# LLM evaluation framework root:

EVALFRMROOT=../AIRUN-Evaluation-Framework

# Python path to access the framework modules:

export PYTHONPATH=$EVALFRMROOT/src

# activate the framework virtual environment:

cd $EVALFRMROOT
source .venv/Scripts/activate
```


TESTROOT points to the root directory of agent tests in AIRUN-Assistants-Benchmark-TestInstructions. 

```bash
TESTROOT=../AIRUN-Assistants-Benchmark-TestInstructions/agentic-workflow-tests
```

The directory structure is currently dictated by the framework: root directory contains test directories with numerical names, for example 0001, 0002, ...

Each test directory contains a test specification `TestSpec.xml` and a draft report  `testing-template.md`.

See [test specification sample](TestSpec.sample.xml).

## Running Test

The agent's performance on the test is assessed in two phases.

The result of the first attempt is assessed to find out how correctly the agent assessed the initial task and whether he started solving it correctly.

The second phase is targeted to evaluate cooperative iterative work of developer with a coding agent and asses how easy to get the final solution of acceptable quality.

### Test Run Setup

Input:
- Test specification

Output:
- Project directory
- Test branch

Act:
- Clone the project repository
- Create a branch to run test in
- Specify test number

```bash
git checkout -b test/agentic/0016/amp/run20250605

TESTNBR=0001
```

### First-Shot Run

#### Solution generation

Input:
- Test specification
- Project directory
- Test branch

Output:
- Git commit hash solution is generated after. For instance: HEAD~1
- Last git commit hash of last solution commit. For instance: HEAD

Act:
- Open the project in IDE
- Submit the task
- Wait the solution is generated
- Record changes to the repository using `git add`, `git rm` and `git mv` commands.
- Commit the changes. For instance: `git commit -m agent/try/0`.
- Fix the generated solutions commits as range of git commit hashes:

```bash
# two commits to extract changes generated by the agent:
COMMIT2=HEAD

COMMIT1=HEAD~`git rev-list --count ${BASE_COMMIT:-b79e68d210c52154bb51b7217de4a22ea66a15d4}..${COMMIT2:-HEAD}`

echo $COMMIT1 $COMMIT2
```

#### Format First Short Changes in LLM-readable format

Input:
- Test specification
- Project directory
- The generated solutions commits as range of git commit hashes. For instance: HEAD~1 HEAD

Output:
- Changes in LLM-readable format: `output.md`

Description:
> Prepare text describing test task, task related files content, agent generated changes in LLM-readable format.
  The test task description and task related file names are read from test specification file.
  Git command are used to recognize the status of changed files between the given commits and to get the  corresponding files changes.

Act:
- Extract changes generated by the agent to the file and add a draft report to it:

```bash
$TESTROOT/utils/format-llm-eval-context.sh $COMMIT1 $COMMIT2 $TESTROOT/$TESTNBR/TestSpec.xml > $TESTROOT/$TESTNBR/output.md && cat $TESTROOT/$TESTNBR/testing-template.md >> $TESTROOT/$TESTNBR/output.md
```

#### Test the First Short Solution

Input:
- Test specification
- Draft `output.md`

Output:
- Evalution-ready `output.md`

Act:
- Perform test steps described in TestPlan section of the test specification.
- Open `output.md` and add the test artifacts (these can be build, run, test logs) and test results.

#### First Short Solution Evaluation by LLM

Input:
- Test specification
- Evalution-ready `output.md`

Output:
- LLM evaluation report of solution by categories: `accuracy.md`, `completeness.md`

Description:
> The evaluation is done by launching `evaluate.py` from AIRUN-Evaluation-Framework. The script produces two files completeness.md, accuracy.md in Markdown format for two hard-coded categories. The report includes the status of each evaluated criteria  (e.g., Pass, Fail),  the model's confidence in the conclusion it has reached (e.g., 100%), and optional explanation how model gets to the conclusion.

Act:
- Extract the criteria to be evaluated from the test specification in the format required by the framework:

```bash
python $TESTROOT/utils/llm-evaluate.py --test-spec $TESTROOT/$TESTNBR/TestSpec.xml --meta-dump-file $TESTROOT/$TESTNBR/meta.yaml
```
- Evaluate the solution by LLM:

```bash
python $EVALFRMROOT/evaluate.py --data-dir $TESTROOT --scenario-ranges $TESTNBR --report-path $TESTROOT/grading-$TESTNBR-`date +%Y%m%d-%H%M%S`.csv
```

#### First Short Evaluation Review by Human

Input:
- LLM evaluation report

Output:
- Reviewed evaluation report

Description:
> A human reviews the status of each LLM evaluated criteria.

Act:

- Convert the framework output to an editable file `TestLog.xml` for human review:

```bash
python $TESTROOT/utils/llm-report-2-test-log.py --llm-reports $TESTROOT/$TESTNBR/completeness.md $TESTROOT/$TESTNBR/accuracy.md --test-log-file $TESTROOT/$TESTNBR/TestLog.xml
```

- Review of test results in `$TESTROOT/$TESTNBR/TestLog.xml`, in case of an incorrect LLM assessment, you can set a different status for the assertion: `<ReviewedStatus>Pass</ReviewedStatus>` or `<ReviewedStatus>Fail</ReviewedStatus>`

#### First Short Scoring

Input:
- Reviewed evaluation report

Output:
- The task solution score

Description:
> Calculation of generated solution score in each category and in total. The results are accumulated in `$TESTROOT/tests-grading.csv`.

```
To make the grade more objective, each criterion is given a weight and the result calculated as a weighted average.
Let's assume that for each criterion we have a result r(i), which can take the value 1 or 0.

We assign a weight w(i) to each criterion:
    HIGH = 1.0
    MEDIUM = 0.5
    LOW = 0.2

Then the final score will be calculated as:
    R = SUM( r(i) * w(i) ) / SUM( w(i) )
```

Act:
 - Calculate the solution score:

```bash
python $TESTROOT/utils/llm-evaluate.py --test-spec $TESTROOT/$TESTNBR/TestSpec.xml --calculate-from-test-log $TESTROOT/$TESTNBR/TestLog.xml --update-test-grades-in $TESTROOT/tests-grading.csv --test-nbr $TESTNBR
```

### Solution Refining Run

#### (Optional, Iterative) Refine the solution

The step is done iteratively in loop. A developer tries to complete the given development either successfully or to prove that further agent-assisted development was not reasonable. It is up to developer how many iteration to take.

##### Review the solution

Input:
- The previously generated solution

Output:
- Developer feedback or comment

Act:
- Review the solution

##### Provide feedback and ask to regenerate

Input:
- Developer feedback or comment

Output:
- Git commit hash solution is generated after. For instance: HEAD~2
- Last git commit hash of last solution commit. For instance: HEAD

Act:
- Submit the feedback to the agent
- Wait the solution is generated
- Record changes to the repository using `git add`, `git rm` and `git mv` commands
- Commit the changes. For instance: `git commit -m agent/try/2`
- Fix the generated solutions commits as range of git commit hashes as described above

#### Format Final Changes in LLM-readable format

Input:
- Test specification
- Project directory
- The generated solutions commits as range of git commit hashes. For instance: HEAD~2 HEAD

Output:
- Changes in LLM-readable format: `output.md`

Description:
> The same to the analogous first-shot step.

#### Test the Final Solution

Input:
- Test specification
- Draft `output.md`

Output:
- Evalution-ready `output.md`

Description:
> The same to the analogous first-shot step.


#### Final Solution Evaluation by LLM

Input:
- Test specification
- Evalution-ready `output.md`

Output:
- LLM evaluation report of solution by categories: `accuracy.md`, `completeness.md`

Description:
> The same to the analogous first-shot step.

#### Final Evaluation Review by Human

Input:
- LLM evaluation report

Output:
- Reviewed evaluation report

Description:
> The same to the analogous first-shot step with additional step described below.

Act:
- Convert the framework output to an editable file `TestLog.xml` for human review
- Review of test results in `$TESTROOT/$TESTNBR/TestLog.xml`
- Register comments/feedbacks in the Refinement section of `TestLog.xml`. For example:

```xml
<Refinement>
    <Comment>
        Could you check V1__initial_schema.sql to have all tables, columns, relations, etc and update it?
    </Comment>
    <Comment>
        Schema-validation: missing table [competition_judges]
    </Comment>
</Refinement>
```

#### Final Solution Scoring

Input:
- Reviewed evaluation report

Output:
- The task solution score

Description:
> The same to the analogous first-shot step.

#### Agent-Assisted Coding Performance Evaluation

Input:
- List of developer feedback or comments or requests (comments/feedbacks in the Refinement section of `TestLog.xml`)

Output:
- Agent-assisted coding performance grade

Description:
> The complexity of obtaining a solution using an agent is estimated based on the number of iterations. It is computed within previous step. The results are accumulated in `$TESTROOT/tests-grading.csv`.

```
The formula of the grade calculation is defined as:

    Grade = M * POW( e, -a * max⁡(0,T−s) )

where:

    M: Maximum grade (e.g., 1),
    T: Try count (number of attempts),
    a: Decay rate (controls how quickly the grade decreases after the 2nd try. e.g., 0.2),
    s: Shift parameter, which shifts the steep drop to occur after s attempts (set s=1 for this case).
```

### Final Test Grade

The final test grade takes into account the grade of the solution obtained after the first attempt, the grade of the final solution, and the complexity of obtaining the final solution.

```
Grade = SUM( C1(i) ) / COUNT( C1(i) ) / 2 + SUM( Cf(i) ) / COUNT( Cf(i) ) / 2 * P

where:

    C1(i): first short grade of category
    Cf(i): final grade of category
    P: performance grade
```

For instance, let's we assume there are grades for two evaluation categories and performance grade:

| Grade | Value |
| --- | --- |
| First-Shot Completeness | 0.60 |
| First-Shot Accuracy | 0.74 |
| Final Completeness | 1.00 |
| Final Accuracy | 0.95 |
| Agent-Assisted Coding Performance | 0.71 |

so the grade is calculated as:

    Grade = (0.60+0.74) / 2 / 2 + (1.0+0.95) / 2 / 2 * 0.71 = 0.681125

### Generate Diff Statistics

Input:
- Test specification
- Project directory
- The generated solutions commits as range of git commit hashes. For instance: HEAD~2 HEAD

Output:
- The solution diff statistics

Description:
> Generating diff statistics between commits as number of modified, added, deleted files and number of inserted, deleted lines.

```bash
$TESTROOT/utils/gen-diff-stat.sh $COMMIT1 $COMMIT2
```

Sample of diff statistics:

```
Files:
3 modified(M)
5 added(A)
0 deleted(D)

Lines:
623 insertions(+)
39 deletions(-)
```

## Agent's Final Rating

Input:
- List of per-test grades: `$TESTROOT/tests-grading.csv`

Output:
- Agent's total grades

Description:
> The agent's final rating is calculated as the arithmetic mean of the test grades.

| Number | Performance | First-Shot Accuracy | First-Shot Completeness | Final accuracy | Final completeness | Grade |
| --- | --- | --- | --- | --- | --- | --- |
| 0001 | 0.449328964 | 0.86 | 0.35 | 0.94 | 0.85 | 0.38 |
| 0002 | 1 | 0.86 | 1.00 | 0.86 | 1.00 | 0.87 |
| 0003 | 0.367879441 | 0.77 | 0.48 | 1.00 | 1.00 | 0.47 |
| | | | | | | *0.57* |


